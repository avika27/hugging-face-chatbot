from model_loader import rf  # your model wrapped in SimpleWrapper
from chat_memory import SlidingMemory

# Set to True if using FLAN-T5, BART, etc.
USE_TEXT2TEXT_MODEL = True  # change to False if you're using text-generation models like GPT-2, Falcon, etc.

def main():
    memory = SlidingMemory(window_size=5)
    print("\nü§ñ Chatbot is ready! Type your message (type 'exit' to quit):\n")

    while True:
        try:
            # Take user input
            user_input = input("You: ").strip()

            if user_input.lower() in ["exit", "quit", "bye"]:
                print("ü§ñ Chatbot: Goodbye! üëã")
                break

            # Build prompt with memory
            prompt = memory.get_prompt(user_input, for_text2text=USE_TEXT2TEXT_MODEL)

            # Show thinking message
            print("\nü§ñ Chatbot (thinking...)\n")

            # Generate response
            response = rf.invoke(prompt)

            # Print chatbot reply
            print(f"ü§ñ Chatbot: {response}\n")

            # Store in memory and logs
            memory.add(user_input, response)

            # Optional: Show sliding memory in terminal
            memory.print_memory()

        except KeyboardInterrupt:
            print("\nü§ñ Chatbot: Session ended manually. üëã")
            break
        except Exception as e:
            import traceback
            print("‚ö†Ô∏è Error:", str(e))
            traceback.print_exc()

if __name__ == "__main__":
    main()
